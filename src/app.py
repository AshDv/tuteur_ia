import requests
import os
import json
import copy
import streamlit as streamlit
from groq import Groq
from dotenv import load_dotenv
from groq.types.chat import ChatCompletionMessageParam
from quiz_agent import QuizAgent

load_dotenv()
class ConversationAgent:
    
    TEACHER_CONTEXT_PATH = os.path.join(os.path.dirname(__file__) + '/../resources/teacher_context.txt')
    QUIZ_CONTEXT_PATH = os.path.join(os.path.dirname(__file__) + '/../resources/quiz_context.txt')

    def __init__(self, quiz_agent: QuizAgent):
        api_key = os.environ.get("GROQ_KEY")
        if not api_key:
            raise ValueError("GROQ_KEY non trouv√©e dans les variables d'environnement.")
            
        self.client = Groq(api_key=api_key)
        self.quiz_agent = quiz_agent
        self.initiate_history()

    @staticmethod
    def read_file(file_path):
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read()

    def initiate_history(self):
        try:
            system_content = self.read_file(self.TEACHER_CONTEXT_PATH)
        except FileNotFoundError:
            system_content = "Vous √™tes un tuteur IA, sage et p√©dagogue."
            
        self.history: list[ChatCompletionMessageParam] = [
            {
                "role": "system",
                "content": system_content
            }
        ]

    def update_history(self, role, content, image_url=None):
        message_data = {"role": role, "content": content}
        if image_url:
            message_data["image_url"] = image_url 
        self.history.append(message_data)
        
    def get_history(self):
        return self.history

    def get_cleaned_api_history(self, include_multimodal_content=False, current_multimodal_content=None):
        messages_to_send = copy.deepcopy(self.history)
        
        for message in messages_to_send:
            if "image_url" in message:
                del message["image_url"]

            if isinstance(message.get("content"), list):
                try:
                    text_content = next(item['text'] for item in message['content'] if item['type'] == 'text')
                    message['content'] = text_content
                except (StopIteration, KeyError):
                    message['content'] = ""

        if include_multimodal_content and current_multimodal_content is not None:
            if messages_to_send[-1]["role"] == "user":
                messages_to_send[-1]["content"] = current_multimodal_content
        
        return messages_to_send

    def ask_llm(self, user_interaction, model, context_text=""):
        teacher_context = self.read_file(self.TEACHER_CONTEXT_PATH)
        system_content = f"{teacher_context}\n\n[CONTEXTE DE COURS]: {context_text}" if context_text else teacher_context
        
        self.update_history(role="user", content=user_interaction)
        
        cleaned_messages = self.get_cleaned_api_history(include_multimodal_content=False)
        cleaned_messages[0] = {"role": "system", "content": system_content}

        try:
            response = self.client.chat.completions.create(
                messages=cleaned_messages,
                model=model
            )
            assistant_content = response.choices[0].message.content
            self.update_history(role="assistant", content=assistant_content)
            return assistant_content
        except Exception as e:
            error_msg = f"‚ùå Ma√Ætre Splinter : Une erreur API est survenue pendant la conversation : {e}"
            self.update_history(role="assistant", content=error_msg)
            return error_msg

    def ask_vision_model(self, user_interaction, images_data, model):

        multimodal_content_api = [{"type": "text", "text": user_interaction}]

        for img in images_data:
            multimodal_content_api.append({
                "type": "image_url",
                "image_url": {
                    "url": f"data:{img['mime']};base64,{img['b64']}",
                },
            })

        first_display_url = images_data[0]['display_url'] if images_data else None
        
        self.update_history(
            role="user", 
            content=user_interaction,
            image_url=first_display_url 
        )
        
        messages_to_send = self.get_cleaned_api_history(
            include_multimodal_content=True,
            current_multimodal_content=multimodal_content_api
        )
        
        try:
            response = self.client.chat.completions.create(
                messages=messages_to_send,
                model=model,
            ).choices[0].message.content
            self.update_history(role="assistant", content=response)
            return response
        except Exception as e:
            error_msg = f"‚ùå Ma√Ætre Splinter : Erreur de vision (API) : {e}"
            self.update_history(role="assistant", content=error_msg)
            return error_msg

    def generate_quiz(self, topic, n_questions, model, difficulty, context_instruction):
        
        prompt_quiz = f"""
            Tu es un professeur expert. Sujet : "{topic}". Niveau : {difficulty}.
            Objectif : G√©n√©rer EXACTEMENT {n_questions} questions.
            {context_instruction}
            ---

            INSTRUCTIONS DE G√âN√âRATION :
            1.  Cr√©e EXACTEMENT {n_questions} questions.
            2.  Assure une VARI√âT√â dans les types de questions ('open' pour les concepts d√©taill√©s, 'qcm' pour la m√©morisation).
            3.  Chaque question doit √™tre ind√©pendante du contexte d'une autre.
            4.  La cl√© "explanation" doit contenir l'explication compl√®te et p√©dagogique de la solution, m√™me pour les questions ouvertes.
            5.  La langue utilis√© pour les question et r√©ponses doit UNIQUEMENT √™tre du fran√ßais.
            
            FORMAT IMP√âRATIF :
            R√©ponds UNIQUEMENT avec un tableau JSON (liste Python) respectant le sch√©ma suivant et les contraintes de cl√©s fournies dans le prompt syst√®me.

            POUR CHAQUE QUESTION 'qcm' :
            -   "type": "qcm"
            -   "choices": Une liste de 4 options (A, B, C, D)
            -   "correct_identifier": La lettre majuscule correcte (A, B, C ou D)

            POUR CHAQUE QUESTION 'open' :
            -   "type": "open"
            -   "correct_identifier": La r√©ponse d√©taill√©e et compl√®te attendue pour la correction.
            """
        
        try:
            quiz_context = self.read_file(self.QUIZ_CONTEXT_PATH)
        except FileNotFoundError:
            quiz_context = "Vous √™tes un expert en formatage JSON strict. R√©pondez UNIQUEMENT avec le tableau JSON demand√©."

        messages_to_send = [
            {"role": "system", "content": quiz_context},
            {"role": "user", "content": prompt_quiz}
        ]
        
        try:
                raw_response = self.client.chat.completions.create(
                    messages=messages_to_send,
                    model=model, 
                ).choices[0].message.content
                
                if raw_response.strip().startswith("```json"):
                    raw_response = raw_response.strip().strip("```json").strip("```").strip()

                quiz_data = json.loads(raw_response)
                
                self.quiz_agent.create_quiz(quiz_data) 
                
                return True

        except json.JSONDecodeError as e:
            error_message = f"Erreur de d√©codage JSON: Le LLM n'a pas retourn√© un format valide. D√©tails: {e}. R√©ponse brute re√ßue: {raw_response[:200]}..."
            print(f"[LOG CONSOLE - QUIZ GENERATION ERROR] {error_message}")
            return error_message
        
        except Exception as e:
            error_message = f"Erreur API/R√©seau: √âchec de la connexion √† Groq ou erreur interne. D√©tails: {e}"
            print(f"[LOG CONSOLE - QUIZ GENERATION ERROR] {error_message}")
            return error_message

    def get_correction_for_final_review(
            self, 
            question_data: dict, 
            user_answer: str, 
            model="openai/gpt-oss-120b"
        ):
        
        q_type = question_data['type']
        correct_identifier = question_data['correct_identifier']
        explanation = question_data['explanation']
        
        if not correct_identifier or not q_type:
            return {
                "score": 0, 
                "feedback": f"‚ùå Le format de la question est bris√© (Donn√©es manquantes).",
                "error_details": question_data
            }
        
        teacher_context = self.read_file(self.TEACHER_CONTEXT_PATH)
        
        if q_type == 'qcm':
            # --- LOGIQUE AM√âLIOR√âE : R√©cup√©ration du texte complet ---
            choices = question_data.get('choices', [])
            full_correct_answer = correct_identifier # Valeur par d√©faut (la lettre)
            
            # On cherche l'option qui commence par la bonne lettre (ex: "A.")
            if choices:
                for choice in choices:
                    if choice.strip().upper().startswith(correct_identifier.strip().upper()):
                        full_correct_answer = choice
                        break
            
            # Calcul du score
            score = 1 if user_answer.strip().upper() == correct_identifier.strip().upper() else 0
            
            # Construction du feedback clair et complet
            if score == 1:
                feedback = f"‚úÖ **Correct !**\n\nVous avez bien identifi√© la r√©ponse : **{full_correct_answer}**.\n\nüí° *{explanation}*"
            else:
                feedback = f"‚ùå **Incorrect.**\n\nLa bonne r√©ponse est : **{full_correct_answer}**.\n\nüí° **Explication :** {explanation}"
                
            return {"score": score, "feedback": feedback}
        
        else:
            # Pour les questions ouvertes, on garde la logique LLM mais on force un format direct
            prompt_correction = f"""
            TACHE : Corrige cette r√©ponse d'√©tudiant de mani√®re DIRECTE et CONCISE.
            
            Question : {question_data.get('question')}
            R√©ponse attendue : '{correct_identifier}'
            R√©ponse de l'√©tudiant : '{user_answer}'
            Explication contextuelle : {explanation}
            
            R√àGLES :
            1. Si la r√©ponse est juste (sens globalement identique), mets score 1. Sinon 0.
            2. Ton feedback doit commencer directement par "Correct" ou "Incorrect".
            3. Donne ensuite la bonne r√©ponse CLAIREMENT sans fioritures.
            4. Finis par une explication simple.
            
            FORMAT DE SORTIE OBLIGATOIRE (JSON pur) :
            {{"score": (int, 0 ou 1), "feedback": (string)}}
            """

            messages_to_send = [
                {"role": "system", "content": teacher_context},
                {"role": "user", "content": prompt_correction} 
            ]

            try:
                raw_response = self.client.chat.completions.create(
                    messages=messages_to_send,
                    model=model,
                ).choices[0].message.content
                
                if raw_response.strip().startswith("```json"):
                    raw_response = raw_response.strip().strip("```json").strip("```").strip()

                return json.loads(raw_response)
            
            except json.JSONDecodeError as e:
                return {"score": 0, "feedback": f"‚ùå Erreur de formatage de la correction. (D√©tails: {raw_response[:50]}...)"}
            except Exception as e:
                return {"score": 0, "feedback": f"‚ùå Erreur API pendant la correction. D√©tails: {e}"}